---
title: "Causal Impact in R with Advertising Data"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    toc_collapsed: true
  pdf_document:
      toc: true
      number_sections: true
  theme: lumen
---
# Things to do
Define the end of calibration period date properly
Try using ZUID instead of fullvisitorid
Try using a longer time period to account for seasonality


# Introduction

This R Markdown document serves as a guide to introduce users to show a POC for LTV

# Required packages
```{r}

library(tidyverse)
library(BTYD)
library(skimr)
library(bigrquery)
library(DataExplorer)
```
Find the LTV of the cohort of new visitors between 2018-11-01 and 2018-12-30, for a period between 2018-11-01 and 2019-02-28

The calibration period will be 2 months, ending on 2019-01-01.

```{r}
start_date = '2018-11-01'
end_date = '2019-02-28'
cohort_end_date = '2018-12-30'
end_of_cal_period = '2019-01-01'

project <- "zsgp-ga-bigq"
targets_dataset <- "94892991"

get_target_query <- paste0(
   "SELECT
  *
FROM
  (
  SELECT
    date,
    fullVisitorId as cust,
    totals.transactionRevenue/1000000 as sales,
    totals.transactions
     FROM `zsgp-ga-bigq.94892991.ga_sessions_*`
    where _TABLE_SUFFIX BETWEEN FORMAT_DATE('%Y%m%d', DATE('", start_date, "'))
    AND FORMAT_DATE('%Y%m%d', DATE('", end_date, "'))
  and
    totals.transactions IS NOT NULL
    ) a
JOIN
  (
  SELECT
    fullVisitorId
  FROM `zsgp-ga-bigq.94892991.ga_sessions_*`
    where _TABLE_SUFFIX BETWEEN FORMAT_DATE('%Y%m%d', DATE('", start_date, "'))
    AND FORMAT_DATE('%Y%m%d', DATE('", cohort_end_date, "'))
  and
    totals.newVisits IS NOT NULL
  #  AND RAND() < 1/50
  GROUP BY
    1
    ) b
ON
  a.cust=b.fullVisitorId
ORDER BY
  date"
  )

# new query (used to create csv export from ZSG BQ)
get_target_query <- paste0(
"SELECT
    date,
    fullVisitorId as cust,
   sum( totals.transactionRevenue/1000000) as sales,
    sum(totals.transactions) as transactions
     FROM `zsgp-ga-bigq.94892991.ga_sessions_*`
    where _TABLE_SUFFIX BETWEEN FORMAT_DATE('%Y%m%d', DATE('2018-10-01'))
    AND FORMAT_DATE('%Y%m%d', DATE('2018-12-31'))
  and
    totals.transactions IS NOT NULL
    group by 1,2"
)

target_data <-
  bq_table_download(bq_project_query(project, get_target_query))

#zsg <- read_csv("zsg_bq.csv", col_types = cols(date = #col_character()))

library(fst)
# write.fst(target_data, "zsg_btyd.fst")
elog <- read.fst("zsg_btyd.fst")

```

## Get descriptive stats on the dataset
```{r}
summary(elog)

skim(elog)
```



## Build a BTYD model

After transforming data, we build a customer-by-sufficient-statistic matrix for the Pareto-NBD model

```{r}
elog <- elog %>% 
  mutate_at(vars(sales), funs(replace(., is.na(.), 0))) %>% 
  filter(sales > 0)

elog <- data.frame(cust = elog$cust, date = elog$date, sales = elog$sales)
#elog$sales <- elog$sales / 10^6

# format dates
elog$date <- as.Date(elog$date, "%Y%m%d")

# set key dates
end.of.cal.period <- as.Date(end_of_cal_period)
T.end <- max(elog$date)
T.star <- as.numeric(T.end - end.of.cal.period)
T.tot <- as.numeric(max(elog$date) - min(elog$date))

dataset <- dc.ElogToCbsCbt(elog, per="day", 
  T.cal=end.of.cal.period)

# estimate model
# https://stackoverflow.com/questions/26280683/error-btyd-pnbd-estimateparameters-l-bfgs-b-needs-finite-values-of-fn
cal.cbs <- dataset[["cal"]][["cbs"]]
params <- pnbd.EstimateParameters(cal.cbs)
# params <- bgnbd.EstimateParameters(cal.cbs)

print(params)
```

```{r}
mape <- function(actual, forecasted) {
    n <- length(actual)
    (1/n) * sum(abs((actual - forecasted) / actual))
}

# calculate incremental and cumulative vectors
T.cal <- cal.cbs[,3]
actual.inc.tracking.data <- cbind(dataset[['cal']][['cbt']], 
  dataset[['holdout']][['cbt']])
actual.inc.tracking.data <- apply(actual.inc.tracking.data, 2, sum)
actual.cum.tracking.data <- cumsum(actual.inc.tracking.data)
expected.cum.trans <- pnbd.ExpectedCumulativeTransactions(params, T.cal, T.tot, length(actual.cum.tracking.data))

# find end of calibration date
end.of.cal.day <- as.numeric(end.of.cal.period - min(elog$date))

total.mape <- mape(actual.cum.tracking.data, expected.cum.trans)
in.sample.mape <- mape(actual.cum.tracking.data[1:end.of.cal.day], expected.cum.trans[1:end.of.cal.day])
out.of.sample.mape <- mape(actual.cum.tracking.data[end.of.cal.day:T.tot], expected.cum.trans[end.of.cal.day:T.tot])
mape.outputs <- data.frame("MAPE"=c(in.sample.mape, out.of.sample.mape, total.mape))
row.names(mape.outputs) <- c("In sample", "Out of sample", "Total")
print(mape.outputs)

```


# Analyse the model

```{r}
print(round(pnbd.Expectation(params, t=365), digits=1))
pnbd.PlotFrequencyInCalibration(params, cal.cbs, 7)
```

# 

```{r}
# cumulative tracking
cum.tracking <- pnbd.PlotTrackingCum(params, T.cal, T.tot,
  actual.cum.tracking.data, xlab="Day")
```

```{r}
# inc tracking
inc.tracking <- pnbd.PlotTrackingInc(params, T.cal, T.tot, 
  actual.inc.tracking.data, xlab="Day", title="Tracking Daily Transactions")
```

```{r}
x.star <- dataset$holdout$cbs[,1]
censor <- 7
cond.expectation <- pnbd.PlotFreqVsConditionalExpectedFrequency(params,
    T.star, cal.cbs, dataset$holdout$cbs[,1], censor)
cond.expectation
```


```{r}
heatmap.palive.data <- matrix(NA, nrow=10, ncol=200)
heatmap.cet.data <- matrix(NA, nrow=10, ncol=200)
for(i in 1:10) {
    heatmap.cet.data[i,] <- pnbd.ConditionalExpectedTransactions(params, T.star=365, x=i, t.x=1:200, T.cal=200)
    heatmap.palive.data[i,] <- pnbd.PAlive(params, x=i, t.x=1:200, T.cal=200)
}
image(heatmap.palive.data, axes=FALSE, xlab="Number of Transactions", ylab="Days since last transaction", main="P(Alive) by Recency and Frequency")
axis(1, at=seq(0,1,.1), labels=0:10)
axis(2, at=seq(0,1,.1), labels=seq(200,0,-20))

image(heatmap.cet.data, axes=FALSE, xlab="Number of Transactions", ylab="Days since last transaction", main="Conditional Expected Transactions by Recency and Frequency")
axis(1, at=seq(0,1,.1), labels=0:10)
axis(2, at=seq(0,1,.1), labels=seq(200,0,-20))
```

# Forecasting Spend
```{r}
#cal.cbt <- dc.CreateSpendCBT(elog, is.avg.spend = FALSE)
cal.cbt <- dc.ElogToCbsCbt(elog, per="day", T.cal=end.of.cal.period, cohort.birth.per=as.Date(cohort_end_date), statistic="total.spend")
cal.cbs <- cal.cbt$cal$cbs
cal.cbt <- cal.cbt$cal$cbt
calculateAvgSpend <- function(cbt.row) {
    purchaseCols = which(cbt.row != 0)
    sum(cbt.row[purchaseCols]) / length(purchaseCols)
}

m.x <- apply(cal.cbt, 1, calculateAvgSpend)
m.x[which(is.na(m.x))] <- 0
spendParams <- spend.EstimateParameters(m.x, cal.cbs[,1])
print(spendParams)
```

```{r}
expected.spend <- spend.plot.average.transaction.value(spendParams, m.x, cal.cbs[,1], title = "Actual vs. Expected Avg Transaction Value Across Customers")
```


```{r}
mape(m.x[which(m.x > 0)], expected.spend)
```

# Compute Model Outputs
Once confident in model fit, the model must be rerun using all of the data. In the previous section, the data from the holdout period was used to assess model fit. In order to generate a true forecast, you must run the model on all of the data. Running the model may take a few minutes.

```{r}
end.of.cal.period <- as.Date(end_date)

dataset <- dc.ElogToCbsCbt(elog, per="day", 
  T.cal=end.of.cal.period)

# estimate model
cal.cbs <- dataset[["cal"]][["cbs"]]
params <- pnbd.EstimateParameters(cal.cbs)

print(params)

```

# Find the Probability that a customer is still alive

In addition to forecasting how many times a new customer will purchase in the next year, you can use the model to forecast how many times each existing customer will purchase in the next year, conditional on their past behavior. You can also compute the probability that they are still alive (p_Alive). Both the expected transactions and probability of being alive for each customer are computed below, conditional on their observed behavior.

```{r}
x <- cal.cbs[, "x"]
t.x <- cal.cbs[, "t.x"]
T.cal <- cal.cbs[, "T.cal"]
d <- .15 # discount rate to be divided by 365, as we are running the model on daily data
discounted_expected_transactions <- pnbd.DERT(params, x, t.x, T.cal, d / 365)
p_Alive <- pnbd.PAlive(params, x, t.x, T.cal)
output <- data.frame(visitorId=dataset$cust.data$cust, discounted_expected_transactions=discounted_expected_transactions, p_Alive=p_Alive)
head(output)
```

Finally, compute the expected spend per transaction and combine this with the expected transactions for a projected future revenue number.

```{r}
expected_spend <- spend.expected.value(spendParams, m.x, cal.cbs[,1])
output$expected_spend <- expected_spend
output$projected_future_revenue <- output$expected_spend * output$discounted_expected_transactions
output[1:20,]
```


# Explore the output dataset
```{r}
summary(output)
skim(output)

```


The output table above can now be imported into BigQuery and analyzed alongside other Google Analytics data.

# Analyzing the data in BigQuery

Fill any missing values with zero
```{r}
output <- output %>% replace_na(list(0))


```

Create a table with the schema that was just created.
Upload job code needs to be updated
```{r}
tablename <- "LTV_output"

dataset <- bq_dataset("zsgp-ga-bigq", "94892991")
bq_table_upload(bq_table(dataset, "output"), 
                output,
                fields = list(bq_field("visitorId","string"),
                              bq_field("discounted_expected_transactions", "integer"),
                              bq_field("p_Alive", "integer"),
                              bq_field("expected_spend", "integer"),
                              bq_field("projected_future_revenue", "integer"))
)


bq_table_delete(bq_mtcars)
```


Continue from here 
https://github.com/googleanalytics/bigquery-export-ipython-notebooks/blob/master/notebooks/customer-base-analysis.ipynb


